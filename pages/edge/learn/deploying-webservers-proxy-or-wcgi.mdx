# Deploying Webservers - Proxy or WCGI

There are two different ways to deploy web servers on Wasmer Edge.

Each comes with it's own advantages and tradeoffs.

## Proxy

In proxy mode, the WebAssembly instance is expected to start an HTTP server on
a specified port (currently defaulting to `80`).

The Edge gateway will then forward (proxy) requests to the inner server, and
send the response back to the client.

A started application will be re-used for multiple requests, and even concurrently
if the application runs a multi-threaded server.
This increases performance, thanks to reduced startup overhead and lack of a
required warmup.

This deployment mode currently requires the [WASIX](https://wasix.org)
WebAssembly toolchain, a superset over [WASI](https://wasi.dev/).
WASIX adds additional functionality to WASI, in particular the ability to listen
on sockets.

To learn how to Deploy a WASIX server, follow the
[Rust HTTP Server Quickstart](../quickstart/http-server.mdx).

To learn more about WASIX and how to build WASIX-enabled applications, see the
[wasix.org](https://wasix.org) website, or the
[WASIX announcement post](https://wasmer.io/posts/announcing-wasix).

## WCGI

WCGI, short for WebassmblyCommonGatewayInterface, uses the tried and true
[CGI](https://en.wikipedia.org/wiki/Common_Gateway_Interface) protocol to process
HTTP requests.

For each incoming request, the gateway will start a brand-new WebAssembly instance,
provide request information through env vars and `stdin`, and then read the
response from stdout.

This means you can use every language and toolchain that compiles to WASI by
simply using a library for CGI, which is available in pretty much every
language.

The primary benefits of this approach:

- Supported by all languages that already compile to standard WASI
- **Isolation**: Each request is handled in a isolated context in a new application
  instance, which is sandboxed.
  This means there is no way for requests to negatively affect each other by
  blocking threads, or for bugs or vulnerabilities introduced by shared state.
- **Perfect scalability**: The gateway starts a new instance for each request,
  so there is no need to worry about concurrency or thread safety.
  The Edge gateway will do the work for you, and scale up dynamically.
  Your instances also stop immediately after the request is served, so you don't
  incur any additional cost from idle instances.

To run WCGI workloads locally, see the [WCGI Runner](/runtime/runners/wcgi) docs.

You can also check out the [WCGI announcement post](https://wasmer.io/posts/announcing-wcgi).
